MACHINE LEARNING
	Przykładowy problem do rozwiązania
		Problem -> Jest miasto, które nawiedza powódź
		Pytanie -> Przewidują, że bedzie 2400mm opadów w tym roku, jakie będą szkody wyrzadzone przez taki opady

	Problem Solving Process
		- Identify data that is relevant to the problem -> 'Features' are catagories of data pints that affect the value of a "label"
			* If the amount of ANNUAL RAINFALLL(independent variable -> 'feature') changes, we will probably see a change in FOOD DAMAGE COSTS(dependent variable -> 'label')
		- Assemble a set of a data related to the problem you're trying to solve -> Datasets almost always clenup or formatting
			* Data on Past Events:
			Year	Total	Raifnall(mm) Flood Damage(milion $)
			2008	250		2.1
			2009	197		1.2
			2010	274		2.5
			2011	291		5.3
			2012	136		0	
			2013	306		6.2
		- Decide on the type of output you are predicting -> Regression used with continuos values, classification used with discrete values
			* We have two common types of output:
				1) Classification -> The value of our labels belong to a discrete set
					- Jego output to dwie przeciwne rzeczy np:
						true/false
						spam/not spam
						score no score

						Przykłady:
							Na podstawie ilosci spędoznych godzin na nauce, czy student zda egzmain, czy nie?
								output to: zda ub nie zda
							Na podsawie contentu emaila, czy trafi do spamu? 
								output to spam lub nie spam
							Na podstawie pozycji poycji, z ktorej strzela piłkarz, trafi cdo bramki?
								output to: trafi lub nie trafi

						Przy classyfication mamy pewne możłiwości, ale nie mamy np. 5.5$, albo 4.33 kg itd. Mamy wybory, ale nie musimy mieć tlyko dwóch wboró, możemy mieć ich wiele.

				2) Regression -> The value of our labels belong to a continuos set
					- Jego output należy do pewnego konituum wartości, np:
						1) Na podstawie rocznika, modelu i marki samochodu jaka jest jego wartość?
							Odpowiedź będzie coś pomiędzy 0zł a 200 000zł (więc jest regression)
						2) Na podtawie ilosci spożytych kalorii danej osoby i ilośći ćwiczen, ile waży?
							Odpowiedź będzie pomiędzy 20kg a 120kg
						4) Na podstawie wyskości drzewa, jaki jest jego wiek?
							Odpowiedź będzie od 0 do 500 lat

		- Based on type of output, pick an algorithm that will determine a correlation between your 'features' and 'labels' -> Many, many different algorithms exist, each with pros and cons
		- Use model generated by algorithm to make a prediction -< Models relate the value of 'features' to the value of 'labels'

	Przykład z PLINK game
		Goal: Given some data about where a ball is dropped from, can we predict what bucket it will end up in?

		"Relevant data" dla naszego zadania:
			- Pozycja zrzucania piłki
			- Numer kosza, do którego wpada piłka
			- Skala "Odbijalności" piłki
			- Rozmiar piłki

		Więc ostatecznie:
			"Features":
				- Drop position
				- Ball Bounciness
				- Ball Size
			"Labels":
				- Bucket a ball lands in

			Changing one of "features" value will probably change "labels" value

		Assemble a set of a data related to the problem - czyli ustalenie w jaki sposób będziemy przechowywać dane do naszej analizy
			- Będizmey miec tablice z tablicami (Array of arrays approach (dobre podejście, chyba najlepsze))
				w pojedyńćzej talibcy (tej wenątrz głównej tablicy) bedzie:
					[dropPosition, bounciness, ball size, bucket]

		Od tego momentu zacznie się pisanie kodu w projekcie, zeby zbeirać dane w naszej aplikacji

		Następny krok to ocenienie jaki mamy tu typu problem (czyli czy bedzie regression or classyfication)
			Będzie tutaj CLASSYFICATION -> ponieważ, jest tu 10 pudełek do któych wada piłka i tylko to, tylko
			te 10 możliwości, więc nie mamy tutaj pewnego rodzaju przedziału tak jak w przykładach z regression. Mamy tylko pewne dostępne opcje, i tyle, w naszym przypadku 10 opcji. Może to się wydawać dziwne, ale trzeba taką metodę przyjąć. Przy classyfication mamy pewne możłiwości, ale nie mamy np. 5.5$, albo 4.33 kg itd. Mamy wybory, ale nie musimy mieć tlyko dwóch wboró, możemy mieć ich wiele.

			The ball can only land in one of these buckets

		Do naszego przypadku użyjemy poniższego algorytmu
			K-Neares Neighbor (knn) -> jeden z wieu algorytmów
				W kursie był krótki opis i porónanie do ptaków. Jeśli widzisz wiele tych smaych ptaków na niebie i nagle dolatuje do nich kolejny bardzo podobny, to można z dużą dozą prawdopodobieńśtwa założyc, że jest tot en sam ptak.

				Jeśli mamy coś takiego
					Drop position:	Bucket:
					298				4
					300				4
					299				4
					301				4
					300				4
				Teraz stawiajać pytanie, do jakiego pudełka wpadnie piłka z pozycji 300?
				Możemy z dużą dozą prawdopodobieństwa powiedzieć, że do pudłęka nr 4.
				KNN algorytm patrzy na pewne podbne inputy (features), bierze pod uwage ich wynik
				po czym mowi, ze skoro w takich pozycjach spadło do pudełka 4, to przy 4 prawdopodobnie też wpadnie do pudełka 4

				Nasza implementacja KNN (with one independent variable)
					Which bucket will a ball go into dropped at 300px? ->
					-> Drop a ball a bunch of times all around the board, record which bucket it goes into ->
					-> For each observation, substract drop point from 300px, take absolute value abs(dropPosition - 300) ->
					-> Sort the result from least to greatest ( tutaj sortujemy pod względem tej wartości po wcyciągnieciu z niej wartości bezwzględnej czyli z abs(dropPosition - 300). Generanei u góry będą wtedy zrzucenia piłki najbliżej pozycji 300px) ->
					-> Look at the "k" top records. What was the most common bucket? "k" oznacza tutaj top 5 lub 6 lub whatever wynikow. Dla k=3 bierzemy 3 wyniki z góry naszej tabeli, i wybeiramy najczęstszy przypadek, czyli do jakeigo pudełka piłka wpada najczeciej. Oczywiście ilość "k" jest bardzo ważna, ale generlanie "k" moze przyjmować wiele wartości w zależności od Ciebie i co chcesz zrobic ->
					-> Wchichever bucket came up most frequently is the one ours will probably go into

			Po początkowej analizie, w naszym przypadku wyszło, że nie była najlepsza. Trzeba pamiętać, że tak się często zdarza i wtedy trzeba wykonać pewne kroki:

				Our Prediction was bad!:

					- Adjust the parameters of the analysis -> np. zmienić parametr "k"
					- Add more features to explain the analysis -> np. dodać "odbijalność" i rozmiar piłki
					- Change the prediction point -> Może z punktem 300 jest coś dziwnego i warto sprawdzić inne pozycje
					- Accept that maybe there isn't good correlation -> Jeśli ciagle się coś nie zgadza, może trzeba znaleźć lepszą korelację

				UWAGA! Zanim podejmiemy jakiś z powyższych kroków potrzeba coś zrobić
				Doing this is pointless if we don't have a good way to compare accuracy with different settings!

				Teraz przykład ze znalezieniem dobrego "k"
					Finding an ideal K
					-> Revord a bunch of data points
					-> Split that data into a 'traning' set and a 'test' set
						example:
						Traning              	Test
						[40, .5, 16, 1]		    [10, .5, 16, 1]
						[140, .5, 16, 2]		[137, .5, 16, 2]
						[250, .5, 16, 2]		[150, .5, 16, 2]
						[250, .5, 16, 2]		[260, .5, 16, 2]
			
					-> For each 'test' record, run KNN using the 'traning' data
					-> Does the result of KNN equal the 'test' record bucket

					Czyli generalnie weżmiemy pierwszy element z test [10, .5, 16, 1]
					i na podstawie danych z Traning bedziemy chcieli przewidzieć gdzie wpadnie piłka z pozycji "10", bo taka pozycja jest w pierwszym elemencie "Test", jeśli algorytm przewidzi, ze wpadnie 1, to mamy pierwszy mały sukces. Taką samą operację zrobimy dla każdego elementu z "Test"
						

            Teraz weźmiemy pod uwagę, więcej zmeinnych:
            	Nasza implementacja KNN (with multiple independent variable)
                    Which bucket will a ball go into dropped at 300px? ->
                    -> Drop a ball a bunch of times all around the board, record which bucket it goes into ->
                    -> TYLKO OBLICZENIA W TYM PUNKCIE SIĘ ZMIENIĄ! - For each observation, substract drop point from 300px, take absolute value abs(dropPosition - 300, boucness - 0.5) ->
                        Wczesniej obliczanie odległosci moglibyśmy rpzedstawić na jednej osi X (one dimentional distance), a teraz te obliczenia będą odległosciach w ukałdzie współrzednych X Y (multi dimentional distance).
                        Odległość będziemy liczyć z pitagorasa, bo to będzie najmniejsza odległość pomiędzy poszczególnymi punktami

                    -> Sort the result from least to greatest ( tutaj sortujemy pod względem tej wartości po wcyciągnieciu z niej wartości bezwzględnej czyli z abs(dropPosition - 300). Generanei u góry będą wtedy zrzucenia piłki najbliżej pozycji 300px) ->
                    -> Look at the "k" top records. What was the most common bucket? "k" oznacza tutaj top 5 lub 6 lub whatever wynikow. Dla k=3 bierzemy 3 wyniki z góry naszej tabeli, i wybeiramy najczęstszy przypadek, czyli do jakeigo pudełka piłka wpada najczeciej. Oczywiście ilość "k" jest bardzo ważna, ale generlanie "k" moze przyjmować wiele wartości w zależności od Ciebie i co chcesz zrobic ->
                    -> Wchichever bucket came up most frequently is the one ours will probably go into

                Jeśli będziemy chcieli uwzględnić 3 zmienne (dropPosition, bouciness, ball size), to będziemy liczyć odległość w 3D

            UWAGA!
                W kursie na początku celowo źle pokazał Osie. Byz a duży rozstaw pomiędzy 0.5 a 0.55 w bounciness.\
                W rzeczywistości jak liczmy odległośc okazuje sie, ze bouciness ma małę znaczenia, bo daje bardzo mały "dodatek" do odległości

            Trzeba więc Znormalizować lub Standaryzować nasze dane!!!

            NORMALIZATION/SCALING oraz STANDARIZATION
                NORMALIZATION/SCALING -> można spotkać dwie nazwy

            WARTO zrobić screeny z tego kursu z tych rysunków itd.

            Poniższy wzór stosujemy do tylko jednej wartości, np. do dropPosition albo do bouciness itd.
                Normalized Dataset = (FeatureValue - minOfFeatureValues) / (maxOfFeatureValues - minOfFeatureValues)

                Przykład jak to wygląda w praktyce:
                     Drop Position                               Normalized Positions
                     200                                          .1
                     150              -> Min: 150 Max: 650        0
                     650                                          1
                     430                                          .56

                Teraz jak by wyglądał kod do tego przy pomocy lodash
                const points = [200, 150, 650, 430];

                const min = _.min(points);
                const max = _.max(points);

                _.map(points, point => {
                    return (point - min) / (max - min); // wyrzuci [0.1, 0, 1, 0.56]
                });

        UWAGA!
        Po tych naszych operacjach, czyli normlalizowaniu danych (dropPoint, bouciness, ballSize), okazuje się, że uzyskujemy gorsze wyniki niż wcześniej!
        Trzeba wiec się zastanowić na ile bouciness i ballSize mają znaczenia. Bardzo możłiwe, ze nie powinny być traktowane równie poważnie co dropPosition

        Feature Selection with KNN
            Changes to DropPosition -> Predictable changes to output
            Changes to Ball Bounciness -> Changes our output but not predictably -> więc powinniśmy inaczej trakować Ball Bounciness

            Feature selection -> Deciding which features to include in analysis

            [
                [300,4],
                [350,5],
                [416,4],
                [722,7]
            ]
            -> Dataset with only drop position -> KNN -> Accuracy of 30%

            [
                [.5,4],
                [.52,5],
                [.53,4],
                [.55,7]
            ]
            -> Dataset with only bounciness -> KNN - Accuracy of 10%

            Patrząc na powyższe przykąłdy możemy dojść do wniosku, że lepiej ignorować bounciness

            W naszym kodzie zrobimy teraz takie zmiany, które pomogą nam zrobić "feature selection",
            bo będą odpalać analizę tlyko dla jednego feature

            Po analizie dla każdego "feature" dochodzimy do wniosku, że analiza TYLKO bouciness lub ballSize,
            jest nam zupełnie niepotrzebna. Nie mówi to nam nic o potenjclanym wyniku.
            Dochdozimy do wniosku, ze w takiej analizie potrzbeujemy tylko dropPosition!
            Mimo tego, ze bouciness i ballSize, moze wpłynąć na wynik, to jednak nie jest to coś co pomoze nam w przewidywaniach.
            (bouciness i ballSize) Ma wpływ, ale ten wpływ nie jest przewidywalny

        The End of the Introduction:
            - Features vs Labels
            - Test vs Training sets of data
            - Feature Normalization
            - Common data structures )arrays of arrays)
            - Feature Selection

        Porównanie Lodash i Tensorflow do naszego użytku

            Lodash:
                Plusy:
                    - Methods for just about everyting we need
                    - Excellent API design (especially chain!)
                    - Skills transferrable to other JS projects
                Minusy:
                    - Extremely slow (relatively)
                    - Not 'numbers' focused
                    - Some things are awkward (getting a columns of values)

            Tensorflow JS:
                Plusy:
                    - Similar API to Lodash
                    - Extremely fast for numeric calculations
                    - Has a "low level" linear algerba API + higher level API for ML
                    - Similar api to numpy - popular Python numerical lib
                Minusy:
                    - Still in active development

        Plan na następną częśc kursu związaną z Tensorflow.js
            - Learn some fundamentals around Tensorflow JS
            - Go through a couple exercises with Tensorflow
            - Rebuild KNN algorithm using Tensorflow
            - Build other algorithms with Tensorflow

            What's the fastest way to learn ML?
            Where are the algorithms and cool examples ?
                Answers:
                -> The fastest way to learn ML is to master fundamental operations around working with data. ->
                -> Strong knowledge of data handling basics makes applying any algorithm trivial.
