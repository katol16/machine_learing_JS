MACHINE LEARNING
	Przykładowy problem do rozwiązania
		Problem -> Jest miasto, które nawiedza powódź
		Pytanie -> Przewidują, że bedzie 2400mm opadów w tym roku, jakie będą szkody wyrzadzone przez taki opady

	Problem Solving Process
		- Identify data that is relevant to the problem -> 'Features' are catagories of data pints that affect the value of a "label"
			* If the amount of ANNUAL RAINFALLL(independent variable -> 'feature') changes, we will probably see a change in FOOD DAMAGE COSTS(dependent variable -> 'label')
		- Assemble a set of a data related to the problem you're trying to solve -> Datasets almost always clenup or formatting
			* Data on Past Events:
			Year	Total	Raifnall(mm) Flood Damage(milion $)
			2008	250		2.1
			2009	197		1.2
			2010	274		2.5
			2011	291		5.3
			2012	136		0	
			2013	306		6.2
		- Decide on the type of output you are predicting -> Regression used with continuos values, classification used with discrete values
			* We have two common types of output:
				1) Classification -> The value of our labels belong to a discrete set
					- Jego output to dwie przeciwne rzeczy np:
						true/false
						spam/not spam
						score no score

						Przykłady:
							Na podstawie ilosci spędoznych godzin na nauce, czy student zda egzmain, czy nie?
								output to: zda ub nie zda
							Na podsawie contentu emaila, czy trafi do spamu? 
								output to spam lub nie spam
							Na podstawie pozycji poycji, z ktorej strzela piłkarz, trafi cdo bramki?
								output to: trafi lub nie trafi

						Przy classyfication mamy pewne możłiwości, ale nie mamy np. 5.5$, albo 4.33 kg itd. Mamy wybory, ale nie musimy mieć tlyko dwóch wboró, możemy mieć ich wiele.

				2) Regression -> The value of our labels belong to a continuos set
					- Jego output należy do pewnego konituum wartości, np:
						1) Na podstawie rocznika, modelu i marki samochodu jaka jest jego wartość?
							Odpowiedź będzie coś pomiędzy 0zł a 200 000zł (więc jest regression)
						2) Na podtawie ilosci spożytych kalorii danej osoby i ilośći ćwiczen, ile waży?
							Odpowiedź będzie pomiędzy 20kg a 120kg
						4) Na podstawie wyskości drzewa, jaki jest jego wiek?
							Odpowiedź będzie od 0 do 500 lat

		- Based on type of output, pick an algorithm that will determine a correlation between your 'features' and 'labels' -> Many, many different algorithms exist, each with pros and cons
		- Use model generated by algorithm to make a prediction -< Models relate the value of 'features' to the value of 'labels'

	Przykład z PLINK game
		Goal: Given some data about where a ball is dropped from, can we predict what bucket it will end up in?

		"Relevant data" dla naszego zadania:
			- Pozycja zrzucania piłki
			- Numer kosza, do którego wpada piłka
			- Skala "Odbijalności" piłki
			- Rozmiar piłki

		Więc ostatecznie:
			"Features":
				- Drop position
				- Ball Bounciness
				- Ball Size
			"Labels":
				- Bucket a ball lands in

			Changing one of "features" value will probably change "labels" value

		Assemble a set of a data related to the problem - czyli ustalenie w jaki sposób będziemy przechowywać dane do naszej analizy
			- Będizmey miec tablice z tablicami (Array of arrays approach (dobre podejście, chyba najlepsze))
				w pojedyńćzej talibcy (tej wenątrz głównej tablicy) bedzie:
					[dropPosition, bounciness, ball size, bucket]

		Od tego momentu zacznie się pisanie kodu w projekcie, zeby zbeirać dane w naszej aplikacji

		Następny krok to ocenienie jaki mamy tu typu problem (czyli czy bedzie regression or classyfication)
			Będzie tutaj CLASSYFICATION -> ponieważ, jest tu 10 pudełek do któych wada piłka i tylko to, tylko
			te 10 możliwości, więc nie mamy tutaj pewnego rodzaju przedziału tak jak w przykładach z regression. Mamy tylko pewne dostępne opcje, i tyle, w naszym przypadku 10 opcji. Może to się wydawać dziwne, ale trzeba taką metodę przyjąć. Przy classyfication mamy pewne możłiwości, ale nie mamy np. 5.5$, albo 4.33 kg itd. Mamy wybory, ale nie musimy mieć tlyko dwóch wboró, możemy mieć ich wiele.

			The ball can only land in one of these buckets

		Do naszego przypadku użyjemy poniższego algorytmu
			K-Neares Neighbor (knn) -> jeden z wieu algorytmów
				W kursie był krótki opis i porónanie do ptaków. Jeśli widzisz wiele tych smaych ptaków na niebie i nagle dolatuje do nich kolejny bardzo podobny, to można z dużą dozą prawdopodobieńśtwa założyc, że jest tot en sam ptak.

				Jeśli mamy coś takiego
					Drop position:	Bucket:
					298				4
					300				4
					299				4
					301				4
					300				4
				Teraz stawiajać pytanie, do jakiego pudełka wpadnie piłka z pozycji 300?
				Możemy z dużą dozą prawdopodobieństwa powiedzieć, że do pudłęka nr 4.
				KNN algorytm patrzy na pewne podbne inputy (features), bierze pod uwage ich wynik
				po czym mowi, ze skoro w takich pozycjach spadło do pudełka 4, to przy 4 prawdopodobnie też wpadnie do pudełka 4

				Nasza implementacja KNN (with one independent variable)
					Which bucket will a ball go into dropped at 300px? ->
					-> Drop a ball a bunch of times all around the board, record which bucket it goes into ->
					-> For each observation, substract drop point from 300px, take absolute value abs(dropPosition - 300) ->
					-> Sort the result from least to greatest ( tutaj sortujemy pod względem tej wartości po wcyciągnieciu z niej wartości bezwzględnej czyli z abs(dropPosition - 300). Generanei u góry będą wtedy zrzucenia piłki najbliżej pozycji 300px) ->
					-> Look at the "k" top records. What was the most common bucket? "k" oznacza tutaj top 5 lub 6 lub whatever wynikow. Dla k=3 bierzemy 3 wyniki z góry naszej tabeli, i wybeiramy najczęstszy przypadek, czyli do jakeigo pudełka piłka wpada najczeciej. Oczywiście ilość "k" jest bardzo ważna, ale generlanie "k" moze przyjmować wiele wartości w zależności od Ciebie i co chcesz zrobic ->
					-> Wchichever bucket came up most frequently is the one ours will probably go into

			Po początkowej analizie, w naszym przypadku wyszło, że nie była najlepsza. Trzeba pamiętać, że tak się często zdarza i wtedy trzeba wykonać pewne kroki:

				Our Prediction was bad!:

					- Adjust the parameters of the analysis -> np. zmienić parametr "k"
					- Add more features to explain the analysis -> np. dodać "odbijalność" i rozmiar piłki
					- Change the prediction point -> Może z punktem 300 jest coś dziwnego i warto sprawdzić inne pozycje
					- Accept that maybe there isn't good correlation -> Jeśli ciagle się coś nie zgadza, może trzeba znaleźć lepszą korelację

				UWAGA! Zanim podejmiemy jakiś z powyższych kroków potrzeba coś zrobić
				Doing this is pointless if we don't have a good way to compare accuracy with different settings!

				Teraz przykład ze znalezieniem dobrego "k"
					Finding an ideal K
					-> Revord a bunch of data points
					-> Split that data into a 'traning' set and a 'test' set
						example:
						Traning              	Test
						[40, .5, 16, 1]		    [10, .5, 16, 1]
						[140, .5, 16, 2]		[137, .5, 16, 2]
						[250, .5, 16, 2]		[150, .5, 16, 2]
						[250, .5, 16, 2]		[260, .5, 16, 2]
			
					-> For each 'test' record, run KNN using the 'traning' data
					-> Does the result of KNN equal the 'test' record bucket

					Czyli generalnie weżmiemy pierwszy element z test [10, .5, 16, 1]
					i na podstawie danych z Traning bedziemy chcieli przewidzieć gdzie wpadnie piłka z pozycji "10", bo taka pozycja jest w pierwszym elemencie "Test", jeśli algorytm przewidzi, ze wpadnie 1, to mamy pierwszy mały sukces. Taką samą operację zrobimy dla każdego elementu z "Test"
						

            Teraz weźmiemy pod uwagę, więcej zmeinnych:
            	Nasza implementacja KNN (with multiple independent variable)
                    Which bucket will a ball go into dropped at 300px? ->
                    -> Drop a ball a bunch of times all around the board, record which bucket it goes into ->
                    -> TYLKO OBLICZENIA W TYM PUNKCIE SIĘ ZMIENIĄ! - For each observation, substract drop point from 300px, take absolute value abs(dropPosition - 300, boucness - 0.5) ->
                        Wczesniej obliczanie odległosci moglibyśmy rpzedstawić na jednej osi X (one dimentional distance), a teraz te obliczenia będą odległosciach w ukałdzie współrzednych X Y (multi dimentional distance).
                        Odległość będziemy liczyć z pitagorasa, bo to będzie najmniejsza odległość pomiędzy poszczególnymi punktami

                    -> Sort the result from least to greatest ( tutaj sortujemy pod względem tej wartości po wcyciągnieciu z niej wartości bezwzględnej czyli z abs(dropPosition - 300). Generanei u góry będą wtedy zrzucenia piłki najbliżej pozycji 300px) ->
                    -> Look at the "k" top records. What was the most common bucket? "k" oznacza tutaj top 5 lub 6 lub whatever wynikow. Dla k=3 bierzemy 3 wyniki z góry naszej tabeli, i wybeiramy najczęstszy przypadek, czyli do jakeigo pudełka piłka wpada najczeciej. Oczywiście ilość "k" jest bardzo ważna, ale generlanie "k" moze przyjmować wiele wartości w zależności od Ciebie i co chcesz zrobic ->
                    -> Wchichever bucket came up most frequently is the one ours will probably go into

                Jeśli będziemy chcieli uwzględnić 3 zmienne (dropPosition, bouciness, ball size), to będziemy liczyć odległość w 3D

            UWAGA!
                W kursie na początku celowo źle pokazał Osie. Byz a duży rozstaw pomiędzy 0.5 a 0.55 w bounciness.\
                W rzeczywistości jak liczmy odległośc okazuje sie, ze bouciness ma małę znaczenia, bo daje bardzo mały "dodatek" do odległości

            Trzeba więc Znormalizować lub Standaryzować nasze dane!!!

            NORMALIZATION/SCALING oraz STANDARIZATION
                NORMALIZATION/SCALING -> można spotkać dwie nazwy

            WARTO zrobić screeny z tego kursu z tych rysunków itd.

            Poniższy wzór stosujemy do tylko jednej wartości, np. do dropPosition albo do bouciness itd.
                Normalized Dataset = (FeatureValue - minOfFeatureValues) / (maxOfFeatureValues - minOfFeatureValues)

                Przykład jak to wygląda w praktyce:
                     Drop Position                               Normalized Positions
                     200                                          .1
                     150              -> Min: 150 Max: 650        0
                     650                                          1
                     430                                          .56

                Teraz jak by wyglądał kod do tego przy pomocy lodash
                const points = [200, 150, 650, 430];

                const min = _.min(points);
                const max = _.max(points);

                _.map(points, point => {
                    return (point - min) / (max - min); // wyrzuci [0.1, 0, 1, 0.56]
                });

        UWAGA!
        Po tych naszych operacjach, czyli normlalizowaniu danych (dropPoint, bouciness, ballSize), okazuje się, że uzyskujemy gorsze wyniki niż wcześniej!
        Trzeba wiec się zastanowić na ile bouciness i ballSize mają znaczenia. Bardzo możłiwe, ze nie powinny być traktowane równie poważnie co dropPosition

        Feature Selection with KNN
            Changes to DropPosition -> Predictable changes to output
            Changes to Ball Bounciness -> Changes our output but not predictably -> więc powinniśmy inaczej trakować Ball Bounciness

            Feature selection -> Deciding which features to include in analysis

            [
                [300,4],
                [350,5],
                [416,4],
                [722,7]
            ]
            -> Dataset with only drop position -> KNN -> Accuracy of 30%

            [
                [.5,4],
                [.52,5],
                [.53,4],
                [.55,7]
            ]
            -> Dataset with only bounciness -> KNN - Accuracy of 10%

            Patrząc na powyższe przykąłdy możemy dojść do wniosku, że lepiej ignorować bounciness

            W naszym kodzie zrobimy teraz takie zmiany, które pomogą nam zrobić "feature selection",
            bo będą odpalać analizę tlyko dla jednego feature

            Po analizie dla każdego "feature" dochodzimy do wniosku, że analiza TYLKO bouciness lub ballSize,
            jest nam zupełnie niepotrzebna. Nie mówi to nam nic o potenjclanym wyniku.
            Dochdozimy do wniosku, ze w takiej analizie potrzbeujemy tylko dropPosition!
            Mimo tego, ze bouciness i ballSize, moze wpłynąć na wynik, to jednak nie jest to coś co pomoze nam w przewidywaniach.
            (bouciness i ballSize) Ma wpływ, ale ten wpływ nie jest przewidywalny

        The End of the Introduction:
            - Features vs Labels
            - Test vs Training sets of data
            - Feature Normalization
            - Common data structures )arrays of arrays)
            - Feature Selection

        Porównanie Lodash i Tensorflow do naszego użytku

            Lodash:
                Plusy:
                    - Methods for just about everyting we need
                    - Excellent API design (especially chain!)
                    - Skills transferrable to other JS projects
                Minusy:
                    - Extremely slow (relatively)
                    - Not 'numbers' focused
                    - Some things are awkward (getting a columns of values)

            Tensorflow JS:
                Plusy:
                    - Similar API to Lodash
                    - Extremely fast for numeric calculations
                    - Has a "low level" linear algerba API + higher level API for ML
                    - Similar api to numpy - popular Python numerical lib
                Minusy:
                    - Still in active development

        Plan na następną częśc kursu związaną z Tensorflow.js
            - Learn some fundamentals around Tensorflow JS
            - Go through a couple exercises with Tensorflow
            - Rebuild KNN algorithm using Tensorflow
            - Build other algorithms with Tensorflow

            What's the fastest way to learn ML?
            Where are the algorithms and cool examples ?
                Answers:
                -> The fastest way to learn ML is to master fundamental operations around working with data. ->
                -> Strong knowledge of data handling basics makes applying any algorithm trivial.

        Tensorflow.js
            js.tensorflow.org -> documentation

            Tensor to javascript obiekt, który trzyma tablice w tablicach (w tych tablicach są nasze dane)

            W Tensorflow będziemy często posługiwac się konkretnym slownictwem. Każdy tensor ma swój wymiar/wymiary, ma sój "Shape" np:
                Dimensions:
                    1 Dimensional:
                        [200, 400, 600]
                    2 Dimensional:
                        [
                            [300, 0.4, 16, 4],
                            [300, 0.4, 16, 4],
                            [300, 0.4, 16, 4],
                            [300, 0.4, 16, 4]
                        ]
                    3 Dimensional:
                        [
                            [
                                [5,5,6]
                            ]
                        ]

                Shape -> How many records in each dimension? (one for each dimension)
                    Ważne tu jest to, ze jak masz np:
                        [
                            [5,10,17],
                            [18,4,2]
                        ]

                        to shape bedzie [2,3], a NIE [2,3,3] ! (One time an each level of a dimension)


            2D is most important dimension we will work with
                 [
                     [300, 0.4, 16, 4],
                     [300, 0.4, 16, 4],    -> [#rows, #columns] -> [3,4]
                     [300, 0.4, 16, 4]
                 ]

                 [#rows, #columns] -> number of rows, number of columns

        Przykłady w kodzie dla TensorFlow

            const data = tf.tensor([1,2,3]); // tf - to referencja do Tensorflow
            const otherData = tf.tensor([4,5,6]);
            data.shape; // [3] -> czyli shape tensora

            // Teraz dodanie każdej wartości tensora do siebie (tzn. do "data" dodamy "otherData")
            data.add(otherData); // zwróci -> [5,7,9]
            // To powyżej to tzw. "Element wise opertion"
            // Bardzo ważna informacja jest taka, że tensorflow.js, nie zmieni orginalnych wartości, czyli:
            data; // zwróci -> [1,2,3]
            otherData; // zwróci -> [4,5,6]

            // inne operacje sub - odejmowanie
            data.sub(otherData); // [-3,-3,-3]

            // mul - mnożenie
            data.mul(otherData);

            // div - dzielenie
            data.div(otherData)

            Jeśli "shapes don't match", to na ogół nie możmey zrobić tych operacji "element wise operation", ale będą często przypadki, gdzie i tak będzie to w jakiś sposób to robić

            Jeszcze przykład "element wise operation" z tensorami 2D
                const data = tf.tensor([
                    [1,2,3],
                    [4,5,6],
                ]);
                const otherData = tf.tensor([
                    [4,5,6],
                    [1,2,3],
                ]);

                data.add(otherData); // zwróci -> [[5,7,9],[5,7,9]]

            Poniżej operacje gdzie "shapes don't match",
            const data = tf([1,2,3]);
            const otherData = tf([4]);

            data.add(otherData); // To się wykona normlanie i zwróci [5,6,7] -> czyli doda do każdego 4

            Proces w którym chcemy wykonać "element wise operation" na tablicach o róznych długosciach nazywammy BROADCASTING
                Broadcasting works when...
                    Take shape of both tensors -> From right to left, the shapes are equal or one is '1'
                    W praktyce to wygląda tak:
                        [1,2,3] + [4] = [5,6,7];

                        teraz porównujemy shape tensora [1,2,3] i [4].
                            niech to będzie x = shape z [1,2,3] a y = shape z [4]
                        jeżeli x === y lub (x===1 lub y===1) to możemy zrobić broadcasting !!!

                        kolejny przykład:
                            1   2   3      -     1       =         0    1    2
                            4   5   6      -     1                 3    4    5
                            shape -> [2,3]     shape -> [2,1]

                            W powyższym przypadku porównujemy 3 z 1 (jeden element równa się jeden -> możńa broadcasting) oraz porównujemy 2 z 2 (są rowne -> wiec tez mozna broadcasting) -> Czyli ostatecznie mozna broadcasting

                        Następny przykład:
                            Najlepiej sprawdzić to w kursie, to jest lekcja nr 38

                            mamy shape [2,3,2] oraz [3,1]
                            będziemy to porównywać w taki sposób
                                2   3   2 -> shape peirwszego
                                    3   1 -> shape drugiego
                            Czyli przesuwamy te shape maksymalnie na prawą stronę! Dalej analizujemy:
                            2 porónujemy do 1 -> mamy jeden wiec mozmey broadcasting
                            3 porównujemy do 3 -> są róne więc mozemy broadcasting
                            2 nie mamy do czego porównać, wiec rezygnujemy z porównywania i wciąż MOŻEMY robić broadcasting!

                            OSTATECZNIE -> Możemy!

                        Ostatni przykład:
                            mamy shape [2,3,2] oraz [2,1]
                            będziemy to porównywać w taki sposób
                                 2   3   2 -> shape peirwszego
                                     2   1 -> shape drugiego
                            2 porónujemy do 1 -> mamy jeden wiec mozemy broadcasting
                            3 porównujemy do 2 -> Nie są róne więc NIE mozemy broadcasting

                            OSTATECZNIE -> Nie możemy!

                    MAłA UWAGA!
                        Tam wypisywaliśy np. że:
                            const data = tf.tensor([1,2,3]); // tf - to referencja do Tensorflow
                            data; // zwróci -> [1,2,3]
                            // to powyzej, nie jest do końca prawdą, tak zwracało w edytorze Stpehena Gridera
                            // Normlanie musisz zrobić data.print(); // bo pamiętaj, ze ten tensor [1,2,3] będzie w obiekcie
                            // Tensorflow wszystkie swoje iformacje trzyma w obiekcie
                            // UWAGA! Nie robimy czegoś takiego console.log(data.print()) -> print robi console.log za nas!


                Tensor Accessors:
                    1D Accessors:
                        const data = tf.tensor([10,20,30]);
                        data.get(0); // zwróci 10, czyli pierwszy element
                        data[0]; // NIE możemy tak robić na tensorach! Zwróci błąd!

                    2D Accessors:
                        const data = tftensor([
                            [10, 20, 30],
                            [40, 50, 60]
                        ]);
                        data.get(0,0); // zwróci 10, czyli pierwszy element
                        data.get(1,0); // zwróci 40

                        Pamiętaj!
                        data.set -> NIE MA "SET"!

                How to access many elements:
                    TensorFlow, daje tu dużo możliwości
                    My użyjemy metody slice(), która przyjmuje dwa parametry (start index, size), gdzie size jest NOT ZERO INDEX (czyli zawsze zayczna się od 1 a nie od 0)
                    Start index ,to coś jakby wyznaczyć z jakiej pozycji zacyznamy (row, column) i to row cloumns bedzie w tablicy w kolejnosci [row, column],
                    w size będzie, ilość row i ilość column (szerokośc kolumn) i te ilosci tez beda w tablicy w kolejnosci [row, column].
                        Przykład:
                            const data - tf.tensor([
                                [10, 20, 30],
                                [40, 50, 60],
                                [20, 40, 60]
                            ]}

                            Teraz załóżmy, że chcemy pobrać tylko środkową kolumnę, to bedzie
                            data.slice([0,1],[3,1]); // zwróci [[20],[50],[40]]

                            Bardzo często będzie cieżko w size uzyskać rozmiar, czy będzię ciężko uzyskać tę "3", z kodu wyzej, generalnei to zły pomysł by ta "3" była hardcoded.
                            Można więc tę "3" uzyskać za pomocą data.shape[0], więc to też zadziała dobrze:
                            data.slice([0,1],[data.shape[0],1]);
                            Można to jednak zrobić łatwiej i wstawić po prostu "-1" w to meijsce gdzie jest "3". To "-1" oznacza "daj mi wszystkie rowsy, jakie znajdziesz od odpowiedniego miejsca"
                            data.slice([0,1],[-1,1]); -> też zadziała! Najprostrzy sposób

                Tensor Concatenation
                    Obrazkowy przykład:
                        10 20 30    C  O  N    70 80 90   W Y N     10 20 30 70 80 90
                        40 50 60    C  A  T    40 50 60    I K      40 50 60 40 50 60

                    Teraz przykład w kodzie:
                        const tensorA = tf.tensor([
                            [1,2,3],
                            [4,5,6]
                        ]);
                        const tensorB = tf.tensor([
                            [7,8,9],
                            [10,11,12]
                        ]);

                        tensorA.concat(tensorB); // Zwróci [[1,2,3],[4,5,6],[7,8,9],[10,11,12]] , co nie jest do końca porządnym, bo chociażby "shape" się zmienia.
                            Tutaj chcemy uzyskać taki efekt:
                                [[1,2,3,4,5,6],[7,8,9,10,11,12]]

                        Czasami możemy chcieć uzyskać ten efekt z: [[1,2,3],[4,5,6],[7,8,9],[10,11,12]] , ale możmey chciec to co powyzej, czyli [[1,2,3,4,5,6],[7,8,9,10,11,12]]
                        Dlatego w metodzie concat, mozemy dodać drugi parametr (1 lub 0, gdzie zero jest defaultowo), aby uzyskać efekt z : [[1,2,3,4,5,6],[7,8,9,10,11,12]]
                             tensorA.concat(tensorB, 1); // zwróci  [[1,2,3,4,5,6],[7,8,9,10,11,12]]
                             Co oznacza ten drugi parametr "1" ?

                            AXIS   <-| 10 20 30    C  O  N    70 80 90   W Y N     10 20 30 70 80 90
                             0     <-| 40 50 60    C  A  T    40 50 60    I K      40 50 60 40 50 60
                                     _______________________________________________________________
                                                                 |
                                                               AXIS 1
                             Wartość "0" oznacza, ze chcemy concatenować po AXIS 0, natomiast "1", ze chcemy concatenwoać po AXIS 1.
                                Czyli dla "0" jest [[1,2,3],[4,5,6],[7,8,9],[10,11,12]]
                                Natomiast dla "1" jest [[1,2,3,4,5,6],[7,8,9,10,11,12]]

                Teraz robimy przykład:
                    Będziemy brać pod uwagę skoki w dal. Taki zestaw danych:
                        Jump1   Jump2   Jump3       Player  Height(cm)
                        70      40      73          1       182
                        62      53      25          2       173
                        61      65      54          3       186
                        59      34      73          4       190

                    Najpierw zrobimy sume tych Jumpów:
                        Jump1   Jump2   Jump3   JumpSum       Player  Height(cm)
                        70      40      73      183              1       182
                        62      53      25      179              2       173
                        61      65      54      170              3       186
                        59      34      73      186              4       190
                    A później zrobimy concatenację JempSUm z Player i Height
                        JumpSum       Player  Height(cm)
                        183              1       182
                        179              2       173
                        170              3       186
                        186              4       190
                    Więc teraz skupimy się na tym, żeby utworzyć taki zestaw danych za pomocą TensorFlow.js

                        const jumpData = tf.tensor([
                            [70, 70, 70],
                            [70, 70, 70],
                            [70, 70, 70],
                            [70, 70, 70],
                        ]);

                        const playerData = tf.tensor([
                            [1, 160],
                            [2, 160],
                            [3, 160],
                            [4, 160],
                        ]);

                        // teraz będziemy sumować wartości wewnątrz tensora (jump1 + jump2 + jump3)
                        // Defaultowo doda wszystkie wartości dlatego jumpData.sum(); // zwróci 840
                        // My chcemy innego rezultatu (sum along axis -> było to cześniej) więc:
                        // Mamy axis 0 pionowo i axis 1 poziomo. Mamy sumwoać poziomo więc będzie:
                            jumpData.sum(1); // zwróci [210,210,210,210]
                        // Teraz concatujemy
                            jumpData.sum(1).concat(playerData); // To zwróci BLĄD -> Error in concat1D: rank(rank -> wymiar) of tensors[1] must be the same as the rank of the rest (1)
                            // W następnej lekcji powiemy co z tym zrobić i ska ten błąd:


